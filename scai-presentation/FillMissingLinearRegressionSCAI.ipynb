{"cells":[{"cell_type":"markdown","metadata":{"id":"o-Zdxb23RRCL"},"source":["# Linear Regression Visualization\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/djlouie/Linear-Regression-Visualization/blob/main/scai-presentation/FillMissingLinearRegressionSCAI.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"5Vl83YvXRfDw"},"source":["## 1. Lets Say We Have Some Data\n","\n","In this case we have some synthetic data where hypothetically the x-axis represents the amount of rain and the y-axis represents the number of banana slugs seen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laPAzUuR4_QE"},"outputs":[],"source":["x_coords = [0.1, 0.3, 0.6, 0.8, 1]\n","y_coords = [0.2, 0.45, 0.60, 0.625, 0.75]"]},{"cell_type":"markdown","metadata":{"id":"7VRpVvozTMb2"},"source":["Plotting this data with `matplotlib` we can see a general positive trend, which makes sense because banana slugs like to come out of the ground when the soil is wet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cadVNgBX5ZdZ"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaW5lAIQ5ePE"},"outputs":[],"source":["plt.scatter(x_coords, y_coords, marker='o', color='b')\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"b8Gm78WtxKXJ"},"source":["## 2. Prediciting an Output from Features\n","Lets say we wanted to predict the *number of banana slugs* given a certain *amount of rain*. Thern we would call the *amount of rain* a *feature* and the *number of banana slugs* we predict a *predicted output*. One way to do that is to manually fit a line, $y = mx + b$, to the data we have already *observed* by experimenting with different $y-intercepts/b$ and $slopes/m$, such as below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYDKepmK5ndZ"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbSi6ay--jX7"},"outputs":[],"source":["# plot data points\n","plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","# plot line\n","x_eq = np.linspace(0, 1.2, 100)\n","y_eq = 0.5*x_eq + 0.25  # y = mx + b\n","plt.plot(x_eq, y_eq, label='y = 0.5x + 0.25', color='b')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S6ARk4atysT9"},"source":["A line is plotted with the equation $y = mx + b$, that means once we have plotted the line manuallly to get our *predicted output* we simply input any input $x$ into the equation $y = mx + b$ that we have fitted to out data.\n","\n","To sereate out our *observed output* and our *predicted output* we will often represent *observed output* with $y$ and *predicted output* with $\\hat{y}$. So our line, which we call a *model* is represented by the equation $\\hat{y} = mx + b$."]},{"cell_type":"markdown","metadata":{"id":"8kR0BHMAw83k"},"source":["## 3. Machine Learning vs. Traditional programming\n","In ML the model learns and infers rules from the data, we do not program specific rules ourselves!\n","\n","That means when we manually programmed the line above that was not machine learning! Instead, we want our computer to learn the *parameters*, $m$ and $b$ on its own!"]},{"cell_type":"markdown","metadata":{"id":"FgOAs2swZgW4"},"source":["\n","![](https://raw.githubusercontent.com/djlouie/Linear-Regression-Visualization/main/images/Trad_Prog_Vs_ML.png)"]},{"cell_type":"markdown","metadata":{"id":"HllwAeLN1Ylx"},"source":["## 4. What things do we need for the computer to learn on its own?\n","Think about it, then open the cell to get the answer.\n"]},{"cell_type":"markdown","metadata":{"id":"kAd5NZIu2C72"},"source":["### Answers:\n","1. A measurement to determine how wrong the line is!\n","2. A method to minimize how wrong the line is!"]},{"cell_type":"markdown","metadata":{"id":"7kP0elLr2Eb9"},"source":["# 5. Residual\n","\n","Below is a plot of a line with a $slope$ of 1 and a $y-intercept$ of 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqzWcjdd_A3j"},"outputs":[],"source":["# plot data points\n","plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","# plot line\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq = 1*x_eq + 0\n","plt.plot(x_eq, y_eq, color='b')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9t79exNY3rVA"},"source":["## 5.1 How do we determine how wrong the the line is?\n","\n","For a specefic observed point we can calculate the *residual* which is represented by:\n","$$Residual = Observed - Predicted$$\n","or\n","$$Residual = y - \\hat{y}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVEMMQJ8C0ga"},"outputs":[],"source":["# plot line\n","m = 1  # slope\n","b = 0  # y-intercept\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq = m*x_eq + b\n","plt.plot(x_eq, y_eq, color='b')\n","\n","# plot residual\n","plt.plot([x_coords[1], x_coords[1]],\n","    [y_coords[1], m*x_coords[1]+b], color='g', label = 'residual')\n","\n","# plot data points\n","plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.legend()\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gV4hM_lX4Mvt"},"source":["The residual for the second point is represented by the green line above, lets calculate it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-3RKH6MEVXG"},"outputs":[],"source":["residual = y_coords[1] - m*x_coords[1]+b\n","print('Observed:', y_coords[1])\n","print('Predicted:', m*x_coords[1]+b)\n","print('Residual:', residual)"]},{"cell_type":"markdown","metadata":{"id":"JpBL88yN4jiw"},"source":["## 6. Sum of Squared Residuals\n","\n","To calculate how \"wrong\" the line is for a single point we can calculate the *residual*, we can also call this meassure of devaition the *Error*. However, how should we calculate how \"wrong\" the line is for all of the points?\n","\n","---\n","\n","If we simply added all the residuals, the residuals above and below the line would cancel out. So, to account for that we can simply square the residuals before adding them up. This has the added benefit that says the line is more \"wrong\" the larger the residual is because its squared. We call this the *sum of squared residuals*:\n","\n","---\n","$$SSR = (Observed_1 - Predicted_1) ^2 + (Observed_1 - Predicted_1) ^2 + \\cdots$$\n","or\n","$$SSR = \\sum_{i=1}^{n} (Observed_i - Predicted_i) ^2$$\n","\n","---\n","In machine learning we call the function that measures how \"wrong\" the line is the *Loss*.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYUlBwUiGjNb"},"outputs":[],"source":["# plot line\n","m = 1\n","b = 0\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq = m*x_eq + b\n","plt.plot(x_eq, y_eq, color='b')\n","\n","# plot residual\n","for i in range(len(x_coords)):\n","    plt.plot([x_coords[i], x_coords[i]],\n","        [y_coords[i], m*x_coords[i]+b], color='g')\n","\n","# plot data points\n","plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yFQLYJ6S68wK"},"source":["## 6.1 Lets create a class version of the model\n","1. Making a class simplifies the code so that when we want to create multiple instances of our linear regression model, we just create a new instance of the class\n","2. We will be using the *PyTorch* library's `tensor` class. This allows us to do the slope and bias calculations without looping like the np-arrays above when we were graphing. It also allows us to use `torch.sum` for the summation in the SSR calculation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiXrn8yxRAUe"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAxZLF2LVAMN"},"outputs":[],"source":["dtype = torch.float"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eab8p78hIgad"},"outputs":[],"source":["class linear_reg:\n","    def __init__(self, m, b):\n","        self.m = torch.tensor(m, requires_grad=True, dtype=dtype)\n","        self.b = torch.tensor(b, requires_grad=True, dtype=dtype)\n","    def forward(self, x):\n","\n","        #################\n","        # Fill this in! #\n","        #################\n","\n","    def ssr(self, y, y_hat):\n","\n","        #################\n","        # Fill this in! #\n","        #################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUA97ZSrRLfy"},"outputs":[],"source":["# convert the data to tensors\n","x_input = torch.tensor(x_coords, requires_grad=True, dtype=dtype)\n","y_observed = torch.tensor(y_coords, requires_grad=True, dtype=dtype)"]},{"cell_type":"markdown","metadata":{"id":"0Z76Alkh8Wyb"},"source":["### 6.2 Understanding SSR's Relationship to the Position of the Line\n","\n","A line has two *parameters* that we can manipulate: *m* and *b*. For the simplicty of explanation, lets simply focus on one of them: *b*.\n","\n","What if we start at a *y-intercept* below the points a slightly increment it until it is above the line? What will the SSR look like?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4OYcTNndIHW"},"outputs":[],"source":["iterations = 40\n","loss_hist = []\n","b_hist = []\n","\n","# create an instance of the class\n","model = linear_reg(m = 0.49, b = -0.8)\n","\n","\n","for i in range(iterations):\n","\n","    # do the forward pass and get the predicted outputs\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # calculate SSR\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # storing the calculated SSR in a list\n","    loss_hist.append(loss.item())\n","\n","    # store y-intercept\n","    b_hist.append(model.b.item())\n","\n","    # increase y-intercept\n","    with torch.no_grad():\n","        model.b += 0.05\n","\n","    # priting the values for understanding\n","    print(f'{i},\\t{loss.item()},\\t{model.b.item()}')\n","\n","b_hist.append(model.b.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvLA13T5eOCW"},"outputs":[],"source":["# Plot the loss values\n","plt.plot(range(iterations), loss_hist)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss vs. Iterations')\n","plt.show()\n","\n","print(f'Final parameters: m = {model.m.item()}, b = {model.b.item()}')"]},{"cell_type":"markdown","metadata":{"id":"szlVd9m0-phM"},"source":["### What we learned:\n","\n","1. We observe that when the line is further away from the points, the higher the SSR or *Loss* is.\n","2. More importantly, the the graph on the right forms a type of *loss landscape* that maps a certain bias to a SSR value that is the shape of a parabola.\n"]},{"cell_type":"markdown","metadata":{"id":"nXhY9Ewv_ESG"},"source":["### 6.3 Lets Make an Animation to Better Visualize that Concept:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcrPA45XoQVh"},"outputs":[],"source":["from matplotlib import animation\n","from IPython.display import HTML\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJ8-FuErexGK"},"outputs":[],"source":["# Create subplots: 2 rows, 1 column\n","fig, axs = plt.subplots(1, 2, figsize=(8, 6))\n","\n","def animate(i):\n","    axs[0].cla()\n","    axs[1].cla()\n","\n","\n","    ####\n","    # Plot first subplot\n","    ####\n","\n","    # define m and b\n","    with torch.no_grad():\n","        m = model.m.detach().numpy()\n","        b = b_hist[i]\n","\n","    # plot line\n","    x_eq = np.linspace(0, 1.05, 100)\n","\n","    y_eq = m*x_eq + b\n","\n","    axs[0].plot(x_eq, y_eq, color='b')\n","\n","    # plot residual\n","    for ind in range(len(x_coords)):\n","        axs[0].plot([x_coords[ind], x_coords[ind]],\n","            [y_coords[ind], m*x_coords[ind]+b], color='g')\n","\n","    # plot data points\n","    axs[0].scatter(x_coords, y_coords, marker='o', color='r')\n","\n","    # labels\n","    axs[0].set_xlabel('Amount of Rain')\n","    axs[0].set_ylabel('Number of Banana Slugs Seen')\n","    axs[0].set_xlim(left=0)\n","    axs[0].set_ylim(bottom=-0.5, top=1.5)\n","    axs[0].set_title('Various y-intercepts')\n","\n","    ####\n","    # Plot second subplot\n","    ####\n","\n","    axs[1].plot(range(len(loss_hist[:i+1])), loss_hist[:i+1])\n","    axs[1].set_xlabel('Iteration')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_title('Loss Over Iterations')\n","\n","anim = animation.FuncAnimation(plt.gcf(), animate, frames=iterations + 1, interval=200)\n","display(HTML(anim.to_jshtml()))\n","\n","# uncomment if you want to save\n","# anim.save('animation_loss.gif', writer='pillow', fps=30)\n","\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"Inid8e54dB9f"},"source":["## **We now know how to quantify how \"wrong\" a line is!**\n","Now we have finished the first step!\n","\n","A key insight here is that this *Loss function* does not only work on a line, it works on any *model* that takes in an input and predicts an output.\n","\n","---\n","\n","In more advanced models, such as neural networks, instead of using *Sum of Squared Residuals* you will often see *Mean Squared Error* which if you understand what the mean is you can see why this would be the equation:\n","\n","$$MSE = \\frac{SSR}{n}$$\n","\n","Remember:\n","$$SSR = \\sum_{i=1}^{n} (Observed_i - Predicted_i) ^2$$\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ssAzR-63fOCP"},"source":["# 7. Gradient Descent\n","\n","Letâ€™s say we are at iteration 15. How do we know if we should increase or decrease the y-intercept?\n","\n","![](https://raw.githubusercontent.com/djlouie/Linear-Regression-Visualization/main/images/Down_Or_Up.png)\n","\n","One might think we should try both directions and do which one is best, but:\n","* We would have to calculate it twice and then back step\n","* We wouldnâ€™t know how much to increase or decrease it by. What if we overshot our correction?\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"QCBwTeLYTzRT"},"source":["### 7.1 Lets Use the Slope!\n","\n","What if we instead calculated ths slope of the loss with respect to b?\n","* A positive slope tells us increasing b increases the loss\n","* A negative slope tells us decreasing b decreases the loss\n","* The slope magnitude (aka absolute value) tells us how sensitive the loss is to a change in b\n","* A larger slope magnitude also tells us we are further from a local minima\n","* Thus in order to adjust b, we can subtract the slope from it, and this can give us a more direct path to minimizing the loss!!!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aOd7_ISvfkQh"},"source":["![](https://raw.githubusercontent.com/djlouie/Linear-Regression-Visualization/main/images/Slope_Derivative_Gradient.png)"]},{"cell_type":"markdown","metadata":{"id":"lidLHekCHPs1"},"source":["### 7.2 Gradient One Step Example\n","\n","For this example we will reduce the number of data points: $(0.3, 0.45), (0.6, 0.6), (0.8, 0.635)$.\n","\n","Let's say at the current step that $m = 0.5$ and $b = 0$. Let's plot that below to see what that looks like:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa5YYq2zG6yv"},"outputs":[],"source":["x_coords_sm = [0.3, 0.6, 0.8]\n","y_coords_sm = [0.45, 0.60, 0.625]\n","x_input_sm = torch.tensor(x_coords_sm)\n","y_obs_sm = torch.tensor(y_coords_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6Uu4RWLJThO"},"outputs":[],"source":["# create an instance of the class\n","model = linear_reg(m = 0.5, b = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KCCnjkaJKoC"},"outputs":[],"source":["# get the model parameters\n","with torch.no_grad():\n","    m = model.m.detach().numpy()\n","    b = model.b.detach().numpy()\n","\n","# plot line\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq = m*x_eq + b\n","plt.plot(x_eq, y_eq, color='b')\n","\n","# plot residual\n","for i in range(len(x_coords_sm)):\n","    plt.plot([x_coords_sm[i], x_coords_sm[i]],\n","        [y_coords_sm[i], m*x_coords_sm[i]+b], color='g')\n","\n","# plot data points\n","plt.scatter(x_coords_sm, y_coords_sm, marker='o', color='r')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"plBm4tMEVxH6"},"source":["For lines, finding the *slope* is the same as finding the *derivative*! More generally, another word for *derivative* is *gradient* which is a more broad term used in multivariate calculus that people in the machine learning community prefer."]},{"cell_type":"markdown","metadata":{"id":"gMGaMn-mWwfg"},"source":["#### 7.2.1 The Math to find the Gradient at this Specific Step by Hand:\n"]},{"cell_type":"markdown","metadata":{"id":"8aUNhIo9Xggl"},"source":["Use the chain rule:\n","$$\\frac{dSSR}{db} = \\frac{dSSR}{dResidual} * \\frac{dResidual}{d\\hat{y}} * \\frac{d\\hat{y}}{db}$$\n","\n","---\n","Calculate the components of the chain rule:\n","$$SSR = \\sum_{i=1}^{n} (Residual_i) ^2$$\n","$$\\frac{dSSR}{dResidual} = \\sum_{i=1}^{n} 2(Residual_i)$$\n","\n","---\n","Calculate the components of the chain rule:\n","$$Residual=y-\\hat{y}$$\n","$$\\frac{dResidual}{d\\hat{y}} = 0 - 1 = -1$$\n","\n","---\n","\n","Calculate the components of the chain rule:\n","$$\\hat{y}=mx+b$$\n","$$\\frac{d\\hat{y}}{db} = 0 + b = b$$\n","\n","---\n","Simplify:\n","$$\\frac{dSSR}{db} = \\sum_{i=1}^{n} [2(Residual_i) * -1]$$\n","\n","---\n","Plug in the points, *m*, and *b*:\n","$$\\frac{dSSR}{db} = \\sum_{i=1}^{n} [2(y_i - (mx_i + b) ) * -1] = \\sum_{i=1}^{n} [-2(y_i - (mx_i + b) )]$$\n","$$\\frac{dSSR}{db} = -2(0.45 - 0.5(0.3)) + -2(0.6 - 0.5(0.6)) + -2(0.625 - 0.5(0.8))$$\n","$$= -0.6 +  -0.6 + -0.45 = \\textbf{-1.65}$$\n"]},{"cell_type":"markdown","metadata":{"id":"LRU1Eh3DXVl8"},"source":["#### 7.2.2\n","\n","Using PyTorch `tensors` has another added benefit. It allows us to easily calculate the gradient of the *Sum of Squared Residuals* with respect to *b* simply by calling `.backward()` on our loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS6jSGX4HUTc"},"outputs":[],"source":["# do the forward pass and get the predicted outputs\n","\n","#################\n","# Fill this in! #\n","#################\n","\n","# calculate SSR\n","\n","#################\n","# Fill this in! #\n","#################\n","\n","# Find the derivative with respect to parameters\n","\n","#################\n","# Fill this in! #\n","#################\n","\n","# Print the derivative\n","print(f'The calculated gradient: {model.b.grad}')"]},{"cell_type":"markdown","metadata":{"id":"DfY9x4cpZvQR"},"source":["As you can see, the gradient we calculated with our code is the same as the one we calculated by hand!"]},{"cell_type":"markdown","metadata":{"id":"uiktUStnaOE0"},"source":["#### 7.2.3 Update the Parameter!\n","\n","In order to update any parameter, as we said above, we must subtract the derivative of the paramter with respect to the loss. However, we do not simply subtract the entirety of the gradient, instead we subtrqact a fraction of it determined by the *learning rate*. In this specific case where we are only updating *b* here is the equation:\n","\n","$$b_{new} = b_{old} - lr * \\frac{dSSR}{db}$$"]},{"cell_type":"markdown","metadata":{"id":"6YQlUMNLei4N"},"source":["Now lets do that with code (Here we set learning rate to 0.1):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAbcYb7QPnzZ"},"outputs":[],"source":["with torch.no_grad():\n","    # Define a learning rate\n","    lr = 0.1\n","\n","    # store old b and m\n","    old_b = model.b.detach().numpy().copy()\n","    m = model.m.detach().numpy()\n","\n","    # update b\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # store new b\n","    new_b = model.b.detach().numpy().copy()"]},{"cell_type":"markdown","metadata":{"id":"KSiRf4kSembt"},"source":["Now the class variables are updated! Lets plot the change to see if out line has got better:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuCZ0Zi5Qemy"},"outputs":[],"source":["# plot line\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq_1 = m*x_eq + new_b\n","plt.plot(x_eq, y_eq_1, color='b')\n","\n","# plot residual\n","for i in range(len(x_coords_sm)):\n","    plt.plot([x_coords_sm[i], x_coords_sm[i]],\n","        [y_coords_sm[i], m*x_coords_sm[i]+new_b], color='g')\n","\n","# plot line\n","x_eq = np.linspace(0, 1.05, 100)\n","y_eq_2= m*x_eq + old_b\n","plt.plot(x_eq, y_eq_2, color='b', alpha=0.5)\n","\n","# plot data points\n","plt.scatter(x_coords_sm, y_coords_sm, marker='o', color='r')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Rain vs. Slugs')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1ArUHWwpfOYT"},"source":["As we can see. the line has moved into a better position that is closer to the points and now has less loss!\n","\n","![](https://raw.githubusercontent.com/djlouie/Linear-Regression-Visualization/main/images/Update_Step.png)\n"]},{"cell_type":"markdown","metadata":{"id":"a6RvY_fBH4Y9"},"source":["## 8. Linear Regression in Action!\n","\n","Every time we update the line on the entire data set we call that one *epoch*. So to do *linear regression*, all we have to do is update the line for many epochs!!!\n","\n","Lets do an example where we use *linear regression* to update $b$ of a perfectly flat line with $m=0$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbeCqBZyRWGn"},"outputs":[],"source":["epochs = 50\n","loss_hist = []\n","b_hist = []\n","step_size = 0.01  # aka learning rate\n","\n","# create an instance of the class\n","model = linear_reg(m == 0.0, b == 0.0)\n","\n","\n","for i in range(epochs):\n","\n","    # do the forward pass and get the predicted outputs\n","    y_hat = model.forward(x_input)\n","\n","    # calculate SSR\n","    loss = model.ssr(y_observed, y_hat)\n","\n","    # storing the calculated SSR in a list\n","    loss_hist.append(loss.item())\n","\n","    # store y-intercept\n","    b_hist.append(model.b.item())\n","\n","    # backward pass on the SSR, calculates the gradient\n","    loss.backward()\n","\n","    # Update the parameters\n","    with torch.no_grad():\n","        model.b -= step_size * model.b.grad\n","\n","    # Zero Out Gradients after one iteration\n","    model.b.grad.zero_()\n","\n","    # priting the values for understanding\n","    print(f'{i},\\t{loss.item()},\\t{model.b.item()}')\n","\n","b_hist.append(model.b.item())"]},{"cell_type":"markdown","metadata":{"id":"AvPkPtkrgtGm"},"source":["#### Let's plot the loss over epochs so we can see that the *model* has \"learned' to decrease the loss:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"It7Z4yjISO0S"},"outputs":[],"source":["# Plot the loss values\n","plt.plot(range(epochs), loss_hist)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss vs. Epochs')\n","plt.show()\n","\n","print(f'Final parameters: m = {model.m.item()}, b = {model.b.item()}')"]},{"cell_type":"markdown","metadata":{"id":"heFW7QikhHgt"},"source":["#### Here is the final result, as we can expect the line is somewhere in the middle:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EQU7N99K9mL"},"outputs":[],"source":["with torch.no_grad():\n","    # plot line\n","    x_eq = np.linspace(0, 1.05, 100)\n","\n","    print(model.m, model.b)\n","\n","    y_eq = model.m*x_eq + model.b\n","    plt.plot(x_eq, y_eq, color='b')\n","\n","    # plot residual\n","    for i in range(len(x_coords)):\n","        plt.plot([x_coords[i], x_coords[i]],\n","            [y_coords[i], model.m*x_coords[i]+model.b], color='g')\n","\n","    # plot data points\n","    plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","# labels\n","plt.xlabel('Amount of Rain')\n","plt.ylabel('Number of Banana Slugs Seen')\n","plt.xlim(left=0)\n","plt.ylim(bottom=0)\n","plt.title('Linear Regression Result')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ex0FmyLehQoW"},"source":["#### Let's create an animation for this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ilmgd4XmP96D"},"outputs":[],"source":["def animate(i):\n","    plt.cla()\n","\n","    # define m and b\n","    with torch.no_grad():\n","        m = model.m.detach().numpy()\n","        b = b_hist[i]\n","\n","    # plot line\n","    x_eq = np.linspace(0, 1.05, 100)\n","\n","    y_eq = m*x_eq + b\n","    plt.plot(x_eq, y_eq, color='b')\n","\n","    # plot residual\n","    for i in range(len(x_coords)):\n","        plt.plot([x_coords[i], x_coords[i]],\n","            [y_coords[i], m*x_coords[i]+b], color='g')\n","\n","    # plot data points\n","    plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","    # labels\n","    plt.xlabel('Amount of Rain')\n","    plt.ylabel('Number of Banana Slugs Seen')\n","    plt.xlim(left=0)\n","    plt.ylim(bottom=0)\n","    plt.title('Linear Regression Result')\n","\n","anim = animation.FuncAnimation(plt.gcf(), animate, frames=epochs + 1, interval=50)\n","display(HTML(anim.to_jshtml()))\n","\n","# uncomment if you want to save\n","# anim.save('animation.gif', writer='pillow', fps=30)\n","\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"KoBWLbnnhtuW"},"source":["## 9. Linear Regression for all Parameters!\n","\n","Now that you learned how to use *linear regression* to optimize one parameter, $b$. It is very simple to go from only optimizing the $b$ to optimizing the $m$ and $b$ simultaneously!\n","\n","Every iteration/step:\n","* Calculate:\n","    * $\\frac{SSR}{b}$ and $\\frac{SSR}{m}$\n","* Update:\n","    * $b_{new} = b_{old} - lr * \\frac{dSSR}{db}$\n","    * $m_{new} = m_{old} - lr * \\frac{dSSR}{dm}$"]},{"cell_type":"markdown","metadata":{"id":"Q-YhuHjhjAqu"},"source":["### 9.1 Let's Do that with Code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TlTLfUOR_QQ"},"outputs":[],"source":["epochs = 100\n","loss_hist = []\n","b_hist = []\n","m_hist = []\n","step_size = 0.02  # aka learning rate\n","\n","# create an instance of the class\n","model = linear_reg(m = 0.0, b = 0.0)\n","\n","\n","for i in range(epochs):\n","\n","    # do the forward pass and get the predicted outputs\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # calculate SSR\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # storing the calculated SSR in a list\n","    loss_hist.append(loss.item())\n","\n","    # store y-intercept\n","    b_hist.append(model.b.item())\n","\n","    # backward pass on the SSR, calculates the gradient\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # Update m AND b\n","\n","    #################\n","    # Fill this in! #\n","    #################\n","\n","    # Zero Out Gradients after one iteration\n","    model.b.grad.zero_()\n","    model.m.grad.zero_()\n","\n","    # priting the values for understanding\n","    if i%2 == 0:\n","        print(f'{i},\\t{loss.item()},\\t{model.m.item()},\\t{model.b.item()}')\n","\n","b_hist.append(model.b.item())\n","m_hist.append(model.m.item())"]},{"cell_type":"markdown","metadata":{"id":"IroITLPtjEmP"},"source":["### 9.2 Plot the Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6y-hY6SW6ns"},"outputs":[],"source":["# Plot the loss values\n","plt.plot(range(epochs), loss_hist)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss vs. Epochs')\n","plt.show()\n","\n","print(f'Final parameters: m = {model.m.item()}, b = {model.b.item()}')"]},{"cell_type":"markdown","metadata":{"id":"iZJ_MGvMjHwj"},"source":["### 9.3 Create the Animation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_UTo_t4XH-p"},"outputs":[],"source":["def animate(i):\n","    plt.cla()\n","\n","    # define m and b\n","    with torch.no_grad():\n","        m = m_hist[i]\n","        b = b_hist[i]\n","\n","    # plot line\n","    x_eq = np.linspace(0, 1.05, 100)\n","\n","    y_eq = m*x_eq + b\n","    plt.plot(x_eq, y_eq, color='b')\n","\n","    # plot residual\n","    for i in range(len(x_coords)):\n","        plt.plot([x_coords[i], x_coords[i]],\n","            [y_coords[i], m*x_coords[i]+b], color='g')\n","\n","    # plot data points\n","    plt.scatter(x_coords, y_coords, marker='o', color='r')\n","\n","    # labels\n","    plt.xlabel('Amount of Rain')\n","    plt.ylabel('Number of Banana Slugs Seen')\n","    plt.xlim(left=0)\n","    plt.ylim(bottom=0)\n","    plt.title('Linear Regression Result')\n","\n","anim = animation.FuncAnimation(plt.gcf(), animate, frames=epochs + 1, interval=50)\n","display(HTML(anim.to_jshtml()))\n","\n","# uncomment if you want to save\n","# anim.save('animation.gif', writer='pillow', fps=30)\n","\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"dRJy1IGabR_f"},"source":["## 10 Multiple Input Variables\n","\n","What if we want to use additional data to predict the number of banana slugs, such as the temperature? Is that possible?\n","\n","Yes!\n","* Using 1 feature to predict 1 output is fitting/optimizing a 2D line.\n","    * $\\hat{y} = mx + b$\n","* Using 2 features to predict 1 output is fitting/optimizing a 3D plane.\n","    * $\\hat{y} = m_1x + m_2y + b$\n","* Using 3 features to predict 1 output is fitting/optimizing a 3+D hyperplane!\n","    * $\\hat{y} = m_1x + m_2y + m_3z + \\cdots + b$\n","\n","Another way you may see this written is instead of $m$ for *slope*, we replace it with $w$ for *weight*. Then instead of calling $b$ the *y-intercept* we will call it the *bias*. Then for any linear regression model with $n$ features the equation is:\n","$$\\hat{y} = \\sum^{n}_{i=1} w_ix_i + b$$\n"]},{"cell_type":"markdown","metadata":{"id":"yAy8pX8RmecB"},"source":["### 10.1 3D Linear Regression Model with 2 Input Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OF8VeX40YC0n"},"outputs":[],"source":["class linear_reg_3D:\n","    def __init__(self, m1, m2, b):\n","        self.m1 = torch.tensor(m, requires_grad=True, dtype=dtype)\n","        self.m2 = torch.tensor(m, requires_grad=True, dtype=dtype)\n","        self.b = torch.tensor(b, requires_grad=True, dtype=dtype)\n","    def forward(self, x, y):\n","        return self.m1*x + self.m2*y + self.b\n","    def ssr(self, y, y_hat):\n","        return torch.sum( (y-y_hat)**2 )"]},{"cell_type":"markdown","metadata":{"id":"3xt4fwKhmqfy"},"source":["### 10.2 The Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QYXHaPUsmxL"},"outputs":[],"source":["x_values = torch.tensor([0.088, 0.472, 0.941, 0.452, 0.092, 0.8, 0.988, 0.12, 0.247, 0.507])\n","y_values = torch.tensor([0.489, 0.563, 0.925, 0.442, 0.535, 0.714, 0.865, 0.027, 0.932, 0.233])\n","z_values = torch.tensor([0.18275, 0.37985, 0.49835, 0.4223, 0.16445, 0.5087, 0.55355, 0.40985, 0.0788, 0.54935])"]},{"cell_type":"markdown","metadata":{"id":"aRa-zmPTmt-3"},"source":["### 10.3 Train the Linear Regression Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJQpL51ytu4G"},"outputs":[],"source":["epochs = 120\n","loss_hist = []\n","b_hist = []\n","m1_hist = []\n","m2_hist = []\n","step_size = 0.02  # aka learning rate\n","\n","# create an instance of the class\n","model = linear_reg_3D(m1 = 0.0, m2 = 0.0, b = 0.0)\n","\n","\n","for i in range(epochs):\n","\n","    # do the forward pass and get the predicted outputs\n","    y_hat = model.forward(x_values, y_values)\n","\n","    # calculate SSR\n","    loss = model.ssr(z_values, y_hat)\n","\n","    # storing the calculated SSR in a list\n","    loss_hist.append(loss.item())\n","\n","    # store y-intercept\n","    b_hist.append(model.b.item())\n","    m1_hist.append(model.m1.item())\n","    m2_hist.append(model.m2.item())\n","\n","    # backward pass on the SSR, calculates the gradient\n","    loss.backward()\n","\n","    # Update m AND b\n","    with torch.no_grad():\n","        model.m1 -= step_size * model.m1.grad\n","        model.m2 -= step_size * model.m2.grad\n","        model.b -= step_size * model.b.grad\n","\n","    # Zero Out Gradients after one iteration\n","    model.b.grad.zero_()\n","    model.m1.grad.zero_()\n","    model.m2.grad.zero_()\n","\n","    # priting the values for understanding\n","    if i%2 == 0:\n","        print(f'{i},\\t{loss.item()},\\t{model.m1.item()},\\t{model.m2.item()},\\t{model.b.item()}')\n","\n","b_hist.append(model.b.item())\n","m1_hist.append(model.m1.item())\n","m2_hist.append(model.m2.item())"]},{"cell_type":"markdown","metadata":{"id":"v0uowo6amzhk"},"source":["### 10.4 Plot the Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDuHn5lu4iD4"},"outputs":[],"source":["# Plot the loss values\n","plt.plot(range(epochs), loss_hist)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss vs. Epochs')\n","plt.show()\n","\n","print(f'Final parameters: m1 = {model.m1.item()}, m2 = {model.m1.item()}, b = {model.b.item()}')"]},{"cell_type":"markdown","metadata":{"id":"MTV_Vca6m27L"},"source":["### 10.5 Plot the Final Result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BzYb9gXxg9t"},"outputs":[],"source":["from matplotlib import cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbMgii7hupDt"},"outputs":[],"source":["fig = plt.figure(figsize=(10, 7))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","with torch.no_grad():\n","    # Plotting 3D points\n","    ax.scatter(x_values, y_values, z_values, c='red')\n","\n","    # plot model\n","    x = np.linspace(0, 1.1, 100)\n","    y = np.linspace(0, 1.1, 100)\n","    x, y = np.meshgrid(x, y)\n","    z = model.m1 * x + model.m2 * y + model.b\n","\n","    # Plot the decision boundary\n","    ax.plot_surface(x, y, z, alpha=0.5, color='blue', cmap=cm.coolwarm)\n","\n","    ax.set_xlabel('X-axis')\n","    ax.set_ylabel('Y-axis')\n","    ax.set_zlabel('Z-axis')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eQaGOBbLm7f7"},"source":["### 10.6 Create the Animation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6wr18F3wh5f"},"outputs":[],"source":["fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","def animate(i):\n","    plt.cla()\n","\n","    # define m and b\n","    with torch.no_grad():\n","        m1 = m1_hist[i]\n","        m2 = m2_hist[i]\n","        b = b_hist[i]\n","\n","    # Plotting 3D points\n","    ax.scatter(x_values, y_values, z_values, c='red')\n","\n","    # plot model\n","    x = np.linspace(0, 1.1, 100)\n","    y = np.linspace(0, 1.1, 100)\n","    x, y = np.meshgrid(x, y)\n","    z = m1 * x + m2 * y + b\n","\n","    # Plot the decision boundary\n","    ax.plot_surface(x, y, z, alpha=0.5, color='blue', cmap=cm.coolwarm)\n","\n","    ax.set_xlabel('X-axis')\n","    ax.set_ylabel('Y-axis')\n","    ax.set_zlabel('Z-axis')\n","    ax.set_xlim(0,1)\n","    ax.set_ylim(0,1)\n","    ax.set_zlim(0,1)\n","\n","anim = animation.FuncAnimation(plt.gcf(), animate, frames=epochs + 1, interval=50)\n","display(HTML(anim.to_jshtml()))\n","\n","# uncomment if you want to save\n","# anim.save('3DLinearReg.gif', writer='pillow', fps=30)\n","\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"poVVeg3VnJiJ"},"source":["## 11. CONGRATULATIONS! You now understand how Linear Regression works ðŸ¥³\n","\n","* Linear as in:\n","    * Not bendy (aka straight) ðŸ˜…\n","* Regression as in:\n","    * It is a type of Supervised Learning (Uses Labeled Data):\n","        * **It is a Regression Problem/Mode**l\n","            * The model predicts a continuous value from an input\n","        * It is not a Classification Problem/Model\n","            * The model predicts a predetermined label/category from an input\n"]}],"metadata":{"colab":{"collapsed_sections":["gMGaMn-mWwfg"],"provenance":[{"file_id":"https://github.com/djlouie/Linear-Regression-Visualization/blob/main/LinearRegressionSCAI.ipynb","timestamp":1727121779109}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
